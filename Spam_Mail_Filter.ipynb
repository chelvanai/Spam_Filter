{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is packages use machine learnig works in spam mail filter\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.classify import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"mail.txt\" #This file is the mail file i just now copy my mail in this file\n",
    "with open(fname) as f:\n",
    "    content = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the body text part in email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are help remove email header and fotter setntence, because i do this work as a common email paltform, so decided to find this kind of divides by below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_divide = [\"Dear Friend,\\n\",\"DEAR FRIEND,\\n\",\"ATTENTION:PRESIDENT,CEO Sir/ Madam.\\n\",\"Dear sir,\\n\",\"Dear Sir/Madam,\\n\"\n",
    "         \"ATTENTION:MADAM/SIR\\n\",\"Dear Friend ,\\n\",'Dear Sir/Madam,\\n','Dear Sir,\\n']\n",
    "\n",
    "bottom_divide = [\"YOUR SINCERELY,\\n\",\"Yours Faithfully,\\n\",\"Your Faithfully,\\n\",\"Best Regards, \\n\",\"Best Regards\\n\",\"Sincerely,\\n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = \"\"\n",
    "bottom = \"\"\n",
    "for i in top_divide:\n",
    "    if i in content:\n",
    "        top = i\n",
    "        \n",
    "for i in bottom_divide:\n",
    "    if i in content:\n",
    "        bottom = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [] # This is the message get list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if the mail only have body part below steps will proced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "if top!=\"\" and bottom!=\"\":\n",
    "    for i in content[content.index(top)+1:content.index(bottom)]:\n",
    "        L.append(i)\n",
    "elif top!=\"\":\n",
    "    for i in content[content.index(top)+1:]:\n",
    "        L.append(i)\n",
    "elif bottom!=\"\":\n",
    "    for i in content[content.index(0):content.index(bottom)]:\n",
    "        L.append(i)\n",
    "else:\n",
    "    L = content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print the body part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " '\\n',\n",
       " 'Hi there,  \\n',\n",
       " '\\n',\n",
       " 'We are halfway through our API lifecycle series and the next session , \" Using the Open API Specification Across the SDLC   \"   is tomorrow, July 24th! We will be discussing the tactical journey of standardizing around Open API and how to scale and enforce design practices across teams in your organization.\\n',\n",
       " '\\n',\n",
       " \"In the past two sessions we covered upgrading from open source to pro tooling and exploring design-first vs code-first approaches to API development.  Don't worry if you missed either of these sessions, just sign-up and we will send you a recording of all the past & upcoming sessions!\\n\",\n",
       " '\\n',\n",
       " 'There are 2 more sessions left in the series, sign up now to not miss out!    Here is an overview of remaining topics we will be discussing:   \\n',\n",
       " '\\n',\n",
       " '    July 24th: Using the Open API Specification Across the SDLC\\n',\n",
       " '    July 31st: Showcasing Teams that Have Put SmartBear Tools into Action\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data dictionary for spam mail find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i will use Naviebase classification algorithms to classficy the spam mail so i got spam mail regular words from internet and make a data dictionary \n",
    "spam_words = ['Act now','Action','Additional income','$$$','100%','Apply now','Bonus','Billing','Buy','Cash','Casino','Certified','Clearance','Collect','Deal','Earn','Extra','Free access','Free gift','Great','Expire','Discount','Double your income/cash','Investment','Money','Now','Refinance','Success','Risk-free','Trial']\n",
    "netural_words = ['area', 'book','Thank you ','business', 'case', 'child', 'company', 'country', 'day', 'eye', 'fact', 'family', 'government', 'group', 'hand', 'home', 'job', 'life', 'lot', 'man', 'money', 'month', 'mother', 'Mr', 'night', 'number', 'part', 'people', 'place', 'point', 'problem', 'program', 'question', 'right', 'room', 'school', 'state', 'story', 'student', 'study', 'system', 'thing', 'time', 'water', 'way', 'week', 'woman', 'word', 'work', 'world', 'year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create NavieBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_features = [(word_feats(spam), 'spam') for spam in spam_words]\n",
    "netural_features = [(word_feats(netural), 'netural') for netural in netural_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = spam_features+netural_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### punctuation remove for body text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here i use python string punctuation for remove punction in every sentencce\n",
    "def clear_punctuation(s):\n",
    "    clear_string = \"\"\n",
    "    for symbol in s:\n",
    "        if symbol not in string.punctuation:\n",
    "            clear_string += symbol\n",
    "    return clear_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find probability of spam mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    spam = 0\n",
    "    nonspam = 0\n",
    "    \n",
    "    words = sentence.split(' ')\n",
    "\n",
    "    for word in words:\n",
    "    \n",
    "        classResult = classifier.classify(word_feats(word))\n",
    "        if classResult == 'spam':\n",
    "            spam = spam+1\n",
    "        if classResult == 'netural':\n",
    "            nonspam = nonspam+1\n",
    "        \n",
    "    return [(float(spam) / len(words)),(float(nonspam) / len(words))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there,  \n",
      "\n",
      "We are halfway through our API lifecycle series and the next session , \" Using the Open API Specification Across the SDLC   \"   is tomorrow, July 24th! We will be discussing the tactical journey of standardizing around Open API and how to scale and enforce design practices across teams in your organization.\n",
      "\n",
      "In the past two sessions we covered upgrading from open source to pro tooling and exploring design-first vs code-first approaches to API development.  Don't worry if you missed either of these sessions, just sign-up and we will send you a recording of all the past & upcoming sessions!\n",
      "\n",
      "There are 2 more sessions left in the series, sign up now to not miss out!    Here is an overview of remaining topics we will be discussing:   \n",
      "\n",
      "    July 24th: Using the Open API Specification Across the SDLC\n",
      "\n",
      "    July 31st: Showcasing Teams that Have Put SmartBear Tools into Action\n",
      "\n",
      "Spam probability:-  0.28452896310039166\n",
      "Non Spam probability:-  0.7154710368996083\n"
     ]
    }
   ],
   "source": [
    "#it is total sapm mail percentage by eveey sentence values\n",
    "total_spam = 0\n",
    "total_nonspam = 0\n",
    "total = 0\n",
    "\n",
    "for i in L:\n",
    "    if i==\"\\n\":\n",
    "        pass\n",
    "    else:\n",
    "        print(i)\n",
    "        x,y = find(clear_punctuation(i))\n",
    "        total_spam = x+total_spam\n",
    "        total_nonspam = y+total_nonspam\n",
    "        total = total+1\n",
    "\n",
    "print(\"Spam probability:- \",total_spam/total)\n",
    "print(\"Non Spam probability:- \",total_nonspam/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i use simple Navie Bayes algorithms if it need more accurecy we need data sets about the mails and those data sets need to be classify and train by our self and after make a deep learning model it will work high accurecy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here may be google wrong or the spam word more need for high accurecy, if the accurecy more need we may increase sapm words but rather than using huge amount of data sets and deep learning model is suitable for this work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
